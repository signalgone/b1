<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Linear model playground - FWL Theorem | Signal Gone</title>
<meta name=keywords content><meta name=description content="Hello, so I have been studying Linear models (including Ordinary Least Squares (OLS)) in the past few weeks. And I came across Frisch–Waugh–Lovell (FWL) Theorem. It basically states that multivariate regression can be deposed into (several) univariate regressions via holding one of the variable constant.
For example, you have a regression, $$y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}$$ So the steps for FWL theorem is
Regress $X_{1}$ on $X_{2}$ and take the residuals Regress $y$ on $X_{2}$ and take the residuals Regress the residuals from step 1 against the residuals from step 2 ($X_{1}-\hat{X}_{1}$ against $y-\hat{y}$) you will get back the same coefficient ($\beta_{1}$) for $X_{1}$ as when regress $y$ on $X_{1}$ and $X_{2}$."><meta name=author content><link rel=canonical href=https://signalgone.github.io/b1/docs/2024-07-07-linearmodelplayground/><link crossorigin=anonymous href=/b1/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://signalgone.github.io/b1/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://signalgone.github.io/b1/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://signalgone.github.io/b1/favicon-32x32.png><link rel=apple-touch-icon href=https://signalgone.github.io/b1/apple-touch-icon.png><link rel=mask-icon href=https://signalgone.github.io/b1/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://signalgone.github.io/b1/docs/2024-07-07-linearmodelplayground/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Linear model playground - FWL Theorem"><meta property="og:description" content="Hello, so I have been studying Linear models (including Ordinary Least Squares (OLS)) in the past few weeks. And I came across Frisch–Waugh–Lovell (FWL) Theorem. It basically states that multivariate regression can be deposed into (several) univariate regressions via holding one of the variable constant.
For example, you have a regression, $$y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}$$ So the steps for FWL theorem is
Regress $X_{1}$ on $X_{2}$ and take the residuals Regress $y$ on $X_{2}$ and take the residuals Regress the residuals from step 1 against the residuals from step 2 ($X_{1}-\hat{X}_{1}$ against $y-\hat{y}$) you will get back the same coefficient ($\beta_{1}$) for $X_{1}$ as when regress $y$ on $X_{1}$ and $X_{2}$."><meta property="og:type" content="article"><meta property="og:url" content="https://signalgone.github.io/b1/docs/2024-07-07-linearmodelplayground/"><meta property="article:section" content="docs"><meta property="article:published_time" content="2024-07-07T16:56:37+08:00"><meta property="article:modified_time" content="2024-07-07T16:56:37+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Linear model playground - FWL Theorem"><meta name=twitter:description content="Hello, so I have been studying Linear models (including Ordinary Least Squares (OLS)) in the past few weeks. And I came across Frisch–Waugh–Lovell (FWL) Theorem. It basically states that multivariate regression can be deposed into (several) univariate regressions via holding one of the variable constant.
For example, you have a regression, $$y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}$$ So the steps for FWL theorem is
Regress $X_{1}$ on $X_{2}$ and take the residuals Regress $y$ on $X_{2}$ and take the residuals Regress the residuals from step 1 against the residuals from step 2 ($X_{1}-\hat{X}_{1}$ against $y-\hat{y}$) you will get back the same coefficient ($\beta_{1}$) for $X_{1}$ as when regress $y$ on $X_{1}$ and $X_{2}$."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Docs","item":"https://signalgone.github.io/b1/docs/"},{"@type":"ListItem","position":2,"name":"Linear model playground - FWL Theorem","item":"https://signalgone.github.io/b1/docs/2024-07-07-linearmodelplayground/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Linear model playground - FWL Theorem","name":"Linear model playground - FWL Theorem","description":"Hello, so I have been studying Linear models (including Ordinary Least Squares (OLS)) in the past few weeks. And I came across Frisch–Waugh–Lovell (FWL) Theorem. It basically states that multivariate regression can be deposed into (several) univariate regressions via holding one of the variable constant.\nFor example, you have a regression, $$y=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}$$ So the steps for FWL theorem is\nRegress $X_{1}$ on $X_{2}$ and take the residuals Regress $y$ on $X_{2}$ and take the residuals Regress the residuals from step 1 against the residuals from step 2 ($X_{1}-\\hat{X}_{1}$ against $y-\\hat{y}$) you will get back the same coefficient ($\\beta_{1}$) for $X_{1}$ as when regress $y$ on $X_{1}$ and $X_{2}$.","keywords":[],"articleBody":"Hello, so I have been studying Linear models (including Ordinary Least Squares (OLS)) in the past few weeks. And I came across Frisch–Waugh–Lovell (FWL) Theorem. It basically states that multivariate regression can be deposed into (several) univariate regressions via holding one of the variable constant.\nFor example, you have a regression, $$y=\\beta_{0}+\\beta_{1}X_{1}+\\beta_{2}X_{2}$$ So the steps for FWL theorem is\nRegress $X_{1}$ on $X_{2}$ and take the residuals Regress $y$ on $X_{2}$ and take the residuals Regress the residuals from step 1 against the residuals from step 2 ($X_{1}-\\hat{X}_{1}$ against $y-\\hat{y}$) you will get back the same coefficient ($\\beta_{1}$) for $X_{1}$ as when regress $y$ on $X_{1}$ and $X_{2}$. Mostly people have actually presented it in non-correlated/independent variables form. It works with correlated variables as well.\nSo first, lets generate some random variables of y, which is the target, and 3 inputs of x variable. Now that we have our random variables, we fit them onto OLS. Next, we shall regress each $X$ on the other two $X$ and get the residual. Then, check their correlation between residual, which is residual_x1 as we trying to find the effect of x1, and the other two variables, x2 and x3\nWe can see that the correlation between residual_x1, x2 and x3 changes, it become uncorrelated from highly correlated between x1, x2 and x3.\nWe regress y on Residual_x1, x2, x3. you will see that the coefficient of Residual_x1 is the same as the coefficient of x1 as before. This is the beauty of FWL theorem. We can use this theorem to study the each input on how they affect other inputs.\nFWL theorem works cause of residuals, which are leftover effects after accounting for all the inputs $X$ , are orthogonal to the predicted target $\\hat{y}$. $$\\epsilon^{T}\\hat{y}=0$$ Which is why the correlation changes between Residual_x1 and x2 and x3.\nReferences The Frisch-Waugh-Lovell Theorem (FWL Theorem) from https://www.hbs.edu/research-computing-services/Shared%20Documents/Training/fwltheorem.pdf\nOrthogonality of residuals in linear regression from https://stats.stackexchange.com/questions/371629/orthogonality-of-residuals-in-linear-regression‌\n","wordCount":"322","inLanguage":"en","datePublished":"2024-07-07T16:56:37+08:00","dateModified":"2024-07-07T16:56:37+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://signalgone.github.io/b1/docs/2024-07-07-linearmodelplayground/"},"publisher":{"@type":"Organization","name":"Signal Gone","logo":{"@type":"ImageObject","url":"https://signalgone.github.io/b1/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://signalgone.github.io/b1/ accesskey=h title="Signal Gone (Alt + H)">Signal Gone</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Linear model playground - FWL Theorem</h1><div class=post-meta><span title='2024-07-07 16:56:37 +0800 +0800'>July 7, 2024</span></div></header><div class=post-content><p>Hello, so I have been studying Linear models (including Ordinary Least Squares (OLS)) in the past few weeks.
And I came across Frisch–Waugh–Lovell (FWL) Theorem. It basically states that multivariate regression can be deposed into (several) univariate regressions via holding one of the variable constant.</p><p>For example, you have a regression,
$$y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}$$
So the steps for FWL theorem is</p><ol><li>Regress $X_{1}$ on $X_{2}$ and take the residuals</li><li>Regress $y$ on $X_{2}$ and take the residuals</li><li>Regress the residuals from step 1 against the residuals from step 2 ($X_{1}-\hat{X}_{1}$ against $y-\hat{y}$)<ul><li>you will get back the same coefficient ($\beta_{1}$) for $X_{1}$ as when regress $y$ on $X_{1}$ and $X_{2}$.</li></ul></li></ol><p>Mostly people have actually presented it in non-correlated/independent variables form. It works with correlated variables as well.</p><p>So first, lets generate some random variables of y, which is the target, and 3 inputs of x variable.
<img loading=lazy src=image.png alt="alt text"></p><p>Now that we have our random variables, we fit them onto OLS.
<img loading=lazy src=image-1.png alt="alt text"></p><p>Next, we shall regress each $X$ on the other two $X$ and get the residual. Then, check their correlation between residual, which is residual_x1 as we trying to find the effect of x1, and the other two variables, x2 and x3<br><img loading=lazy src=image-2.png alt="alt text"></p><p>We can see that the correlation between residual_x1, x2 and x3 changes, it become uncorrelated from highly correlated between x1, x2 and x3.</p><p>We regress y on Residual_x1, x2, x3.
<img loading=lazy src=image-3.png alt="alt text">
you will see that the coefficient of Residual_x1 is the same as the coefficient of x1 as before. This is the beauty of FWL theorem. We can use this theorem to study the each input on how they affect other inputs.</p><p>FWL theorem works cause of residuals, which are leftover effects after accounting for all the inputs $X$ , are orthogonal to the predicted target $\hat{y}$. $$\epsilon^{T}\hat{y}=0$$ Which is why the correlation changes between Residual_x1 and x2 and x3.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>The Frisch-Waugh-Lovell Theorem (FWL Theorem) from <a href=https://www.hbs.edu/research-computing-services/Shared%20Documents/Training/fwltheorem.pdf>https://www.hbs.edu/research-computing-services/Shared%20Documents/Training/fwltheorem.pdf</a></p><p>Orthogonality of residuals in linear regression from <a href=https://stats.stackexchange.com/questions/371629/orthogonality-of-residuals-in-linear-regression>https://stats.stackexchange.com/questions/371629/orthogonality-of-residuals-in-linear-regression</a>‌</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://signalgone.github.io/b1/>Signal Gone</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>