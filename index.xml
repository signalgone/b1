<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Signal Gone</title><link>https://signalgone.github.io/b1/</link><description>Recent content on Signal Gone</description><generator>Hugo -- 0.128.2</generator><language>en-us</language><lastBuildDate>Sun, 07 Jul 2024 22:30:37 +0800</lastBuildDate><atom:link href="https://signalgone.github.io/b1/index.xml" rel="self" type="application/rss+xml"/><item><title>Linear model playground - FWL Theorem</title><link>https://signalgone.github.io/b1/docs/linearmodelplayground/</link><pubDate>Sun, 07 Jul 2024 22:30:37 +0800</pubDate><guid>https://signalgone.github.io/b1/docs/linearmodelplayground/</guid><description>Hello, so I have been studying Linear models (including Ordinary Least Squares (OLS)) in the past few weeks. And I came across Frisch–Waugh–Lovell (FWL) Theorem. It basically states that multivariate regression can be deposed into (several) univariate regressions via holding one of the variable constant.
For example, you have a regression, $$y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}$$ So the steps for FWL theorem is
Regress $X_{1}$ on $X_{2}$ and take the residuals Regress $y$ on $X_{2}$ and take the residuals Regress the residuals from step 1 against the residuals from step 2 ($X_{1}-\hat{X}_{1}$ against $y-\hat{y}$) you will get back the same coefficient ($\beta_{1}$) for $X_{1}$ as when regress $y$ on $X_{1}$ and $X_{2}$.</description></item><item><title>About Me</title><link>https://signalgone.github.io/b1/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://signalgone.github.io/b1/about/</guid><description>Hello, I am currently still a student studying economics and quantitative finance. Let&amp;rsquo;s explore the world of quantitative finance together.</description></item></channel></rss>