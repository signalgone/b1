<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Modelling on Signal Gone</title>
    <link>http://localhost:1313/b1/tags/modelling/</link>
    <description>Recent content in Modelling on Signal Gone</description>
    <generator>Hugo -- 0.128.2</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 07 Jul 2024 22:30:37 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/b1/tags/modelling/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear model playground - FWL Theorem</title>
      <link>http://localhost:1313/b1/docs/linearmodelplayground/</link>
      <pubDate>Sun, 07 Jul 2024 22:30:37 +0800</pubDate>
      <guid>http://localhost:1313/b1/docs/linearmodelplayground/</guid>
      <description>Hello, so I have been studying Linear models (including Ordinary Least Squares (OLS)) in the past few weeks. And I came across Frisch–Waugh–Lovell (FWL) Theorem. It basically states that multivariate regression can be deposed into (several) univariate regressions via holding one of the variable constant.
For example, you have a regression, $$y=\beta_{0}+\beta_{1}X_{1}+\beta_{2}X_{2}$$ So the steps for FWL theorem is
Regress $X_{1}$ on $X_{2}$ and take the residuals Regress $y$ on $X_{2}$ and take the residuals Regress the residuals from step 1 against the residuals from step 2 ($X_{1}-\hat{X}_{1}$ against $y-\hat{y}$) you will get back the same coefficient ($\beta_{1}$) for $X_{1}$ as when regress $y$ on $X_{1}$ and $X_{2}$.</description>
    </item>
  </channel>
</rss>
